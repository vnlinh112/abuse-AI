{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab 1 Tasks. Adversarial ML I","provenance":[{"file_id":"1aFxqxW2oOOShKHMEEvPwZ8pEowIzePf2","timestamp":1597103848788},{"file_id":"1Y6tGjJSSYo73RdtCNKPatO7qkvi6jb73","timestamp":1597102860625}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RBoSGTfJXq4h","colab_type":"text"},"source":["**FIT3183 Malicious AI & Dark Side Security**\n","# Lab 1: Adversarial Machine Learning I (Tasks)\n","\n","In this lab, we are looking at 4 simple methods to generate adversarial examples with explanation on how they works and detailed instruction for you to actually implement it.\n","\n","There are 2 tasks:\n","* Implement black-box attack methods: **Semantic Attack** and **Noise Attack**\n","* Implement white-box attack methods: **FGSM Attack** and **FGVM Attack**\n","\n","üëâ ***Copy this Colab notebook to your Drive***, read the instruction and fill the missing code in the functions `xxx_attack` and `test_xxx_attack`. After that, run those cells and see if you can successfully fool the deep learning model! You can also run the test with more images.\n","\n","*Note: If you are new to Google Colab and Pytorch: please read this [Pre-lab Activities](https://drive.google.com/file/d/1aBOkxvGxkOeZlZxYQy0xiEzdEwU-C1DX/view?usp=sharing) first.*\n","\n","<small>*Prepared by [Linh Vu](mailto:linh.vu@monash.edu) (Lab Tutor) Aug 2020.*"]},{"cell_type":"markdown","metadata":{"id":"kS4iDAF3gv-K","colab_type":"text"},"source":["### Helper functions\n","\n","Here are some helper fuctions for image loading, preprocessing, classification, transformation and visualization.\n","\n","* `load_and_preprocess(path)`: Load image, convert to a tensor and normalize it. \n","> *About Normalization*: ResNet model of PyTorch uses pre-trained weights from Google and they expect inputs with pixel values in between -1 to 1. So the inputs must be normalized with below given mean and standard deviation (for the sake of uniformity): `MEAN = [0.485, 0.456, 0.406]`,\n","`STD = [0.229, 0.224, 0.225]`.\n","* `classify(img)`: One call to classify an image, from image, image url or file path.\n","* `visualize(x, x_adv, perturb=False)`: Display the clean image and the adversarial image side by side with the predicted labels.\n","\n","* `NET`: ResNet model with pretrained weights on ImageNet dataset.\n","* `LABELS`: the labels of ImageNet dataset."]},{"cell_type":"code","metadata":{"id":"dXtVUrkEbsKj","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchvision import datasets, models\n","from torchvision import transforms as T\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import PIL\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","# imagenet labels\n","LABELS_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json'\n","LABELS = {int(key):value[1] for (key, value)\n","          in requests.get(LABELS_URL).json().items()}\n","\n","NET = models.resnet18(pretrained=True)\n","NET.eval()\n","\n","# Normalization: ResNet model of PyTorch uses pre-trained weights from Google \n","# and they expect inputs with pixel values in between -1 to 1. So the inputs must be  \n","# normalized with below given mean and standard deviation (for the sake of uniformity). \n","\n","MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","PREPROCESS =   T.Compose([T.Resize(256),\n","                          T.CenterCrop(224),\n","                          T.ToTensor(),\n","                          T.Normalize(MEAN, STD)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cCRyLW8fNIc","colab_type":"code","colab":{}},"source":["def load_and_preprocess(path:str):\n","  \"\"\"\n","  Load image, convert to a tensor and normalize it.\n","  Input:\n","    path: str - file path or url\n","  Return:\n","    img: tourch.Tensor\n","  \"\"\"\n","  fname = path.split('/')[-1]\n","  if path.startswith('http'): # is url\n","    img = Image.open(BytesIO(requests.get(path).content)).convert('RGB')\n","  else:\n","    img = Image.open(path).convert('RGB')\n","  img = PREPROCESS(img).unsqueeze(0)\n","  return img\n","\n","\n","def classify(img:str):\n","  \"\"\"\n","  One call to classify an image. \n","  Input: \n","    img: PIL.Image.Image or str - image, file path or url to an image\n","  Return: \n","    label, val: the predicted label and confidence value\n","  \"\"\"\n","  # Getting the image from `path`\n","  if isinstance(img, str):\n","    img = load_and_preprocess(img)\n","  elif isinstance(img, Image.Image):\n","    img = PREPROCESS(img).unsqueeze(0)\n","\n","  # Getting the image from net\n","  val, pred = torch.max(NET(img), dim=1)\n","  val = val.item() \n","  pred = pred.item()\n","  label = LABELS[pred]\n","  return label, val\n","\n","\n","def tensor_to_image(x, denormalize=True):\n","  \"\"\"\n","  Transform tensor object to numpy array (image data).\n","  Input:\n","    x: torch.Tensor, tensor image\n","    denormalize: need to do reverse of normalization or not\n","  Return:\n","    numpy.array: image data\n","  \"\"\"\n","  x = x.squeeze(0)     #remove batch dimension # B x C x H x W ==> C x H x W\n","  if denormalize: \n","    x = x.mul(torch.FloatTensor(STD).view(3,1,1)).add(torch.FloatTensor(MEAN).view(3,1,1)) \n","  x = np.transpose(x.numpy() , (1,2,0))   # C x H x W  ==>   H x W x C\n","  x = np.clip(x, 0, 1)\n","  return x\n","\n","\n","def visualize(x, x_adv, perturb=False):\n","  \"\"\"\n","  Display the clean image and the adversarial image side by side with the predicted labels.\n","  Input:\n","    x, x_adv: (x:torch.Tensor, label:str, val:float) - tuple of tensor image, label and confidence value \n","              of clean example and adversarial example\n","    perturb: (grad:torch.Tensor, epsilon:float) - tuple of tensor gradients of \n","              loss data and epsilon value to display as perturb\n","  \"\"\"\n","  figure, ax = plt.subplots(1,3, figsize=(18,8))\n","  \n","  ax[0].imshow(tensor_to_image(x[0])); ax[0].axis('off')\n","  ax[0].set_title('Clean Example', fontsize=20)\n","  ax[0].text(0.5,-0.13, \"Prediction: {}\\n Probability: {:.2f}\".format(x[1], x[2]), \n","              size=15, ha=\"center\", transform=ax[0].transAxes)\n","  \n","  if perturb:\n","    ax[0].text(1.1,0.5, \"+{}*\".format(round(perturb[1],3)), \n","              size=15, ha=\"center\", transform=ax[0].transAxes)\n","    ax[1].imshow(tensor_to_image(perturb[0], denormalize=False));  ax[1].axis('off')\n","    ax[1].set_title('Perturbation', fontsize=20)\n","    ax[1].text(1.1,0.5, \" = \", size=15, ha=\"center\", transform=ax[1].transAxes)\n","    i = 2\n","\n","  else:\n","    i = 1; ax[2].axis('off')\n","  \n","  ax[i].imshow(tensor_to_image(x_adv[0])); ax[i].axis('off')\n","  ax[i].set_title('Adversarial Example', fontsize=20)\n","  ax[i].text(0.5,-0.13, \"Prediction: {}\\n Probability: {:.2f}\".format(x_adv[1], x_adv[2]), \n","              size=15, ha=\"center\", transform=ax[i].transAxes)\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3iDv7pGhhom7","colab_type":"text"},"source":["## Task 1: Implement black-box attack methods\n","\n","* Prepare a list of test images, for example: animals, vehicles, objects, ...\n","* Implement **Semantic Attack** and **Noise Attack**\n","* Test the attack methods with your data"]},{"cell_type":"code","metadata":{"id":"N5NAelcMicmw","colab_type":"code","colab":{}},"source":["## YOUR CODE HERE ##\n","\n","# Prepare a list of test image urls (animals, vehicles, objects, ...). Note that the image url should end with .jpg or .png\n","# test_images = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OyUDws_neiut","colab_type":"text"},"source":["### Semantic Attack\n","\n","To do semantic attack, we generate negative images - the adversarial examples that semantically represent the same objects that human can easily recognize but the model fails to classify them correctly.\n","\n","Reference: https://arxiv.org/pdf/1703.06857.pdf\n","\n","Hint: In case the data is centered with `mean = 0` or in the interval of `[-1, 1]`, the negative image can be made by simple negation of the pixel."]},{"cell_type":"code","metadata":{"id":"LwMazYKneQGM","colab_type":"code","colab":{}},"source":["def semantic_attack(x):\n","  \"\"\"\n","  Input: \n","    x:  torch.Tensor - tensor image\n","  Return: \n","    x_adv: torch.Tensor - negative image, the adversarial example\n","  \"\"\"\n","\n","  ## YOUR CODE HERE ##\n","  # Generate the negative tensor x_adv of the image by negating its pixels (1 line)\n","\n","  ## END YOUR CODE HERE ##\n","\n","  return x_adv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j3o_KEv5O4G_","colab_type":"text"},"source":["Assumed that the model can always correctly classify the clean images. Write the test function to test the attack method with a given image url: \n","* Read the image from the image url and classify it.\n","* Generate the adversarial image and classfify it.\n","* Compare the predictions of the model and return whether the attack is successful or not."]},{"cell_type":"code","metadata":{"id":"wuh_ouZElt03","colab_type":"code","colab":{}},"source":["def test_semantic_attack(img_url):\n","  \"\"\"\n","  Input:\n","    img_url: str - url of the image\n","  Return:\n","    bool: result of the attack:\n","        True: \"Sucsess\" - if the model is fooled by adversarial example,\n","        False: \"Fail\" - otherwise\n","  \"\"\"\n","  # Get image tensor from url and preprocess\n","  x = load_and_preprocess(img_url)\n","  \n","  # Classify the clean image\n","  x_pred, x_pred_prob = classify(x)\n","\n","  ## YOUR CODE HERE ##\n","\n","  # Generate the adversarial image from the image tensor (1 line)\n","\n","  # Classify the adversarial image (1 line)\n","  # x_adv_pred, x_adv_pred_prob = ...\n","\n","  # Visualize the clean image, adversarial image and model predictions (1 line)\n","  # visualize(x=(x, x_pred, x_pred_prob), x_adv=... )\n","  \n","  ## END YOUR CODE HERE ##\n","\n","  # Compare the model predictions and return the result message\n","  if x_pred!= x_adv_pred:\n","    print('Sementic Attack: Success!')\n","    return True\n","  print('Sementic Attack: Fail...')\n","  return False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XP_57tshoIoo","colab_type":"code","colab":{}},"source":["test_semantic_attack('https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEY6wqaFl7ho","colab_type":"code","colab":{}},"source":["test_semantic_attack('https://ai.stanford.edu/~jkrause/cars/car1.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaZjhUg-aq2E","colab_type":"code","colab":{}},"source":["test_semantic_attack('https://miro.medium.com/max/3840/1*NcqhsFhED_W9OnyI0ZO3jA.jpeg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQmAE-cBlrtZ","colab_type":"text"},"source":["Now let's test the attack method on your data and calculate the success rate:"]},{"cell_type":"code","metadata":{"id":"P7JFOjWOlm7i","colab_type":"code","colab":{}},"source":["## YOUR CODE HERE ##\n","\n","# Run the test attack on your data test_images and calculate the success_rate\n","\n","# Print the success_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXQc0A7w9pYx","colab_type":"text"},"source":["### Noise Attack\n","\n","Noise is meaningless numbers put together, such that there is really no object present inside it. It is a random arrangement of pixels containing no information. In torch, we create this ‚Äúnoise‚Äù by using the [torch.randn()](https://pytorch.org/docs/master/generated/torch.randn.html) function, which returns a tensor filled with random numbers from a normal distribution (with mean 0 and standard deviation 1).\n","\n","Noise is weak attack that just picks a random point in the attacker's action space. When combined with an attack bundling function, this can be used to implement random search.\n","\n","References:\n","* https://arxiv.org/abs/1802.00420 recommends random search to help identify gradient masking \n","* https://openreview.net/forum?id=H1g0piA9tQ recommends using noise as part of an attack building recipe combining many different optimizers to yield a strong optimizer."]},{"cell_type":"code","metadata":{"id":"Xvlmebv48noa","colab_type":"code","colab":{}},"source":["def noise_attack(x, epsilon=0.7):\n","  \"\"\"\n","  input: \n","    x:  torch.Tensor - tensor image\n","    epsilon: float - epsilon value in range [0; 1]. The larger the epsilon,\n","         the more noticeable the perturbations.\n","  return: \n","    x_adv: torch.Tensor - perturbed image, the adversarial example\n","  \"\"\"\n","\n","  ## YOUR CODE HERE ##\n","  # Create the tensor noise by torch.randn() with the same shape of x and multiply by the epsilon (1 line)\n","\n","  # Generate the perturbed image x_adv by adding the noise to x (1 line)\n","\n","  ## END YOUR CODE HERE ##\n","  \n","  return x_adv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5d2alOhSOmWR","colab_type":"text"},"source":["Assumed that the model can always correctly classify the clean images. Write the test function to test the attack method with a given image url and epsilon value:\n","* Read the image from the image url and classify it.\n","* Generate the adversarial image with the epsilon value and classfify it.\n","* Compare the predictions of the model and return whether the attack is successful or not."]},{"cell_type":"code","metadata":{"id":"MgB6ht1t9G6-","colab_type":"code","colab":{}},"source":["def test_noise_attack(img_url, epsilon=0.7):\n","  \"\"\"\n","  Input:\n","    img_url: str - url of the image\n","  Return:\n","    bool: result of the attack:\n","        True: \"Sucsess\" - if the model is fooled by adversarial example,\n","        False: \"Fail\" - otherwise\n","  \"\"\"\n","  # Get image tensor from url and preprocess\n","  x = load_and_preprocess(img_url)\n","\n","  # Classify the clean image\n","  x_pred, x_pred_prob = classify(x)\n","\n","  ## YOUR CODE HERE ##\n","\n","  # Generate the adversarial image from the image tensor and epsilon (1 line)\n","\n","  # Classify the adversarial image (1 line)\n","  # x_adv_pred, x_adv_pred_prob = ...\n","\n","  # Visualize the clean image, adversarial image and model predictions (1 line)\n","  # visualize(x=(x, x_pred, x_pred_prob), x_adv=...)\n","  \n","  ## END YOUR CODE HERE ##\n","\n","  # Compare the model predictions and return the result message\n","  if x_pred!= x_adv_pred:\n","    print('Noise Attack: Success!')\n","    return True\n","  print('Noise Attack: Fail...')\n","  return False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FovMpzL79NLv","colab_type":"code","colab":{}},"source":["test_noise_attack('https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lrhr2k9-IIR","colab_type":"code","colab":{}},"source":["test_noise_attack('https://ai.stanford.edu/~jkrause/cars/car1.jpg', epsilon=0.75)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDGgQmB7-Xoh","colab_type":"code","colab":{}},"source":["test_noise_attack('https://miro.medium.com/max/3840/1*NcqhsFhED_W9OnyI0ZO3jA.jpeg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAJROa_0qSD8","colab_type":"text"},"source":["Now let's test the attack method on your data and calculate the success rate:"]},{"cell_type":"code","metadata":{"id":"YvsOO1xzqSvK","colab_type":"code","colab":{}},"source":["## YOUR CODE HERE ##\n","\n","# Run the test attack on your data test_images and calculate the success_rate\n","\n","# Print the success_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8vqFPwbt743","colab_type":"text"},"source":["You can see that the success rate is not high since noise attack, or black-box attack in general, is weak. However, this simple method is the basic idea for many other adversarial attack methods. Let's take a look at more advanced attack methods in Activity 2."]},{"cell_type":"markdown","metadata":{"id":"gQBGmZPVt55W","colab_type":"text"},"source":["## Task 2: Implement white-box attack methods\n","\n","* Implement **FGSM Attack** \n","* Implement **FGVM Attack** based on FGSM Attack\n","* Test the attack methods with your data from Task 1"]},{"cell_type":"markdown","metadata":{"id":"oLxHREe-5dUO","colab_type":"text"},"source":["### FGSM Attack\n","\n","One of the first and most popular adversarial attacks to date is referred to as the Fast Gradient Sign Method (FGSM) and is described by Goodfellow et. al. in Explaining and Harnessing Adversarial Examples. The attack is remarkably powerful, and yet intuitive. It is designed to attack neural networks by leveraging the way they learn, gradients. The idea is simple, rather than working to minimize the loss by adjusting the weights based on the backpropagated gradients, the attack adjusts the input data to maximize the loss based on the same backpropagated gradients. In other words, the attack uses the gradient of the loss w.r.t the input data, then adjusts the input data to maximize the loss.\n","\n","The formula to find adversarial example is as follows:\n","$$ X^{adv} = X + \\epsilon sign(\\nabla_X J(X, Y_{true})$$\n","Here, <br>\n","X = original (clean) input <br>\n","$ X_{adv}$ = adversarial input (intentionally designed to be misclassified by our model) <br>\n","$ \\epsilon $ = epsilon, the magnitude of adversarial perturbation  <br>\n","$ \\nabla_X J(X, Y_{true}) $ = gradient of loss function w.r.t to input (X)\n","\n","Reference: https://arxiv.org/abs/1412.6572"]},{"cell_type":"code","metadata":{"id":"MjEplPoYMWYB","colab_type":"code","colab":{}},"source":["def fgsm_attack(x, epsilon, data_grad):\n","  \"\"\"\n","  Input:\n","    x: torch.Tensor - tensor image\n","    epsilon: float - epsilon value in range [0; 1]. The larger the epsilon,\n","         the more noticeable the perturbations.\n","    data_grad: torch.Tensor - data gradient\n","  Return:\n","    x_adv: torch.Tensor - tensor image of the adversarial example\n","  \"\"\"\n","  ## YOUR CODE HERE ##\n","  # Calculate the sign of data gradient (1 line)\n","  \n","  # Create the perturbation by multiplying the gradient sign and epsilon (1 line)\n","\n","  # Generate the perturbed image by adding the perturbation (1 line)\n","\n","  ## END YOUR CODE HERE ##\n","\n","  return x_adv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QNUQamoOSoU","colab_type":"text"},"source":["Assumed that the model can always correctly classify the clean images. Write the test function to test the attack method with different values of epsilon."]},{"cell_type":"code","metadata":{"id":"lIZ5YBAYSxUT","colab_type":"code","colab":{}},"source":["def test_fgsm_attack(img_url, epsilon=0.01):\n","  \"\"\"\n","  Input:\n","    img_url: str - url of the image\n","    epsilon: float or [float], epsilon value(s)\n","  Return:\n","    float: success rate, result of the attack.\n","          1 means the model is 100% fooled by adversarial example\n","  \"\"\"\n","  # Get image tensor from url and preprocess\n","  x = load_and_preprocess(img_url)\n","\n","  # Require calculating the gradient of the variable - important for attack\n","  x.requires_grad = True \n","  \n","  # Forward pass the data through the model\n","  output = NET(x)\n","  \n","  # Get the predicted label with highest probability\n","  x_pred_prob, y_true = output.max(1, keepdim=True)\n","  x_pred_prob = x_pred_prob.item()\n","  y_true = y_true.item()\n","  x_pred = LABELS[y_true]\n","  target = torch.LongTensor([y_true])\n","  target.requires_grad = False\n","\n","  # Calculate the loss\n","  loss = torch.nn.CrossEntropyLoss()\n","  loss_cal = loss(output, target)\n","\n","  # Backward pass the data to get gradients\n","  loss_cal.backward(retain_graph=True)\n","\n","  # Collect data gradient of image\n","  x_grad = x.grad.data\n","\n","  success = 0\n","\n","  # Perform attack with different epsilons\n","  if not isinstance(epsilon, list):\n","    epsilon = [epsilon]\n","  for eps in epsilon:\n","    # Generate the adversarial image and re-classify\n","    x_adv = fgsm_attack(x=x.data, epsilon=eps, data_grad=x_grad)\n","    output_adv = NET(x_adv)\n","    x_adv_pred_prob, x_adv_pred = output_adv.max(1, keepdim=True)\n","    x_adv_pred_prob = x_adv_pred_prob.item()\n","    x_adv_pred = x_adv_pred.item()\n","    x_adv_pred = LABELS[x_adv_pred]\n","\n","    # Compare the results to count the success cases\n","    if x_pred!= x_adv_pred:\n","      success += 1\n","\n","    # Visualize the clean image, adversarial image and model predictions\n","    visualize(x=(x.data, x_pred, x_pred_prob), perturb=(x_grad.sign(), eps),\n","              x_adv=(x_adv, x_adv_pred, x_adv_pred_prob))\n","\n","  success_rate = success/len(epsilon)\n","  if success_rate > 0:\n","    print('FGSM Attack: {:.1f}% Success!'.format(100*success_rate))\n","  else:\n","    print('FGSM Attack: Fail...')\n","  return success_rate  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLBBaP3gXrU-","colab_type":"code","colab":{}},"source":["img_url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'\n","test_fgsm_attack(img_url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3gbuLvKfb0C","colab_type":"text"},"source":["<p style=\"font-size:18px; font-family:open sans; line-height:1.2\">As you can see, the generated adversarial image is visually indistinguishable from the original image but the model classifies it as Saluki. Here is an example of a Saluki dog.\n","\n","![Saluki dog](https://newcastlebeach.org/images/saluki.jpg)\n","\n","Now, let us generate several adversarial images with different values of epsilon. Notice that as we increase the value of epsilon the adversarial image becomes distinguishable from the original one."]},{"cell_type":"code","metadata":{"id":"mBAbH_7PcYDx","colab_type":"code","colab":{}},"source":["test_fgsm_attack(img_url, epsilon=[0.0001, 0.004, 0.01, 0.3, 0.5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvBk8aCGg8vn","colab_type":"text"},"source":["<p style=\"font-size:18px; font-family:open sans; line-height:1.2\">For very small value of epsilon, class doesn't change. But it decreases the probability. An alternative way is to use raw gradient (not sign) without any constraint (epsilon). It is called as Fast Gradient Value Method."]},{"cell_type":"markdown","metadata":{"id":"N-0cs0aXsaou","colab_type":"text"},"source":["Now let's test the attack method with your data and calculate the success rate:"]},{"cell_type":"code","metadata":{"id":"MQxGOyiZs2bR","colab_type":"code","colab":{}},"source":["## YOUR CODE HERE ##\n","\n","# Run the test attack on your data test_images and calculate the success_rate\n","\n","# Print the success_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgXj8m_dxKYE","colab_type":"text"},"source":["### FGVM Attack\n","\n","Fast Gradient Value method is similar to FGSM but it replaces the sign of the gradient with the raw gradient: Œ∑ = ‚àáxJ(Œ∏, x, l). **Fast Gradient Value\n","method has no constraints (epsilon) on each pixel and can generate images with a larger local difference.**\n","\n","The formula to find adversarial example is as follows:\n","$$ X^{adv} = X + \\nabla_X J(X, Y_{true})$$\n","Here, <br>\n","X = original (clean) input <br>\n","$ X_{adv}$ = adversarial input (intentionally designed to be misclassified by our model) <br>\n","$ \\nabla_X J(X, Y_{true}) $ = gradient of loss function w.r.t to input (X)"]},{"cell_type":"code","metadata":{"id":"lbOGGfYvgOOv","colab_type":"code","colab":{}},"source":["def fgvm_attack(x, data_grad):\n","  \"\"\"\n","  Input:\n","    x: torch.Tensor - tensor image\n","    data_grad: torch.Tensor - data gradient\n","  Return:\n","    x_adv: torch.Tensor - tensor image of the adversarial example\n","  \"\"\"\n","  ## YOUR CODE HERE ##\n","\n","  # Generate the perturbed image by adding the data grient value (1 line)\n","\n","  ## END YOUR CODE HERE ##\n","  return x_adv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqsKd3tFOJUa","colab_type":"text"},"source":["Assumed that the model can always correctly classify the clean images. Write the test function to test the attack method, which is similar to FGSM but without epsilon."]},{"cell_type":"code","metadata":{"id":"7kkgR4PRxjWv","colab_type":"code","colab":{}},"source":["def test_fgvm_attack(img_url):\n","  \"\"\"\n","  Input:\n","    img_url: str - url of the image\n","  Return:\n","    bool: result of the attack:\n","        True: \"Sucsess\" - if the model is fooled by adversarial example,\n","        False: \"Fail\" - otherwise\n","\n","  The steps are similar to the test_fgsm_attack but: \n","    (1) use the data gradient value instead of its signs to create the perturbation\n","    (2) there is no epsilon \n","  \"\"\"\n","\n","  ## YOUR CODE HERE ##\n","\n","\n","\n","  ## END YOUR CODE HERE ##\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vf3guXJ9xqDX","colab_type":"code","colab":{}},"source":["test_fgvm_attack(img_url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiJGGNxTtMjs","colab_type":"text"},"source":["Now let's test the attack method on your data and calculate the success rate:"]},{"cell_type":"code","metadata":{"id":"ag-dsUFxaMJ1","colab_type":"code","colab":{}},"source":["## YOUR CODE HERE ##\n","\n","# Run the test attack on your data test_images and calculate the success_rate\n","\n","# Print the success_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAd8N4yJLO60","colab_type":"text"},"source":["## What's next?\n","\n","Now you understand the basic ideas and steps to generate the adversarial samples to attack the model. Let's try to evaluate the effectiveness of those attack methods on a model by generating adversarial data from a whole dataset and performing the attack. \n","\n","Reference: https://pytorch.org/tutorials/beginner/fgsm_tutorial.html"]},{"cell_type":"code","metadata":{"id":"RcBINZUetq-M","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}